<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      /* Sans: Lato, Fira Sans, Fira Sans Condensed;  Serif Sans: Merriweather, Lora, Galdeano*/
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 16px
    }
    
    strong {
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    heading {
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700
    }
    
    name {
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 36px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/ico" href="favicon.ico">
  <title>Xuejian Rong</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href="https://fonts.googleapis.com/css?family=Galdeano" rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Xuejian Rong</name>
              </p>
              <p>I am a researcher at <a href="http://media-lab.ccny.cuny.edu">Media Lab</a> of <a href="http://www.ccny.cuny.edu">The City College</a>, <a href="http://cuny.edu">City University of New York</a>, where I work in the intersection of Deep Learning, Computer Vision, and Image Processing. I am finishing a Ph.D. advised by Prof. <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>. Currently my research interests mainly focus on inference and learning for scene text detection and recognition in the wild, and visual-linguistic understanding on scene text Images.
              </p>
              <p>
                My thesis proposal is entitled "Deep Features for Context-aware Text Extraction" and the slides are available <a href="https://www.dropbox.com/s/s4dlcuqwt3vml3x/2nd.Exam.Xuejian.Rong.4.28.17.slides.pdf?dl=0">here</a>. I interned at <a href="">Siemens Corporate Research</a> and worked on the visual representation learning for novel view synthesis. I obtained the B.E. degree from <a href="http://iao.nuaa.edu.cn">Nanjing University of Aeronautics and Astronautics</a> at 2013.
              </p>
              <p align=center>
                <a href="mailto:xrong@ccny.cuny.edu">Email</a> &nbsp/&nbsp
                <a href="resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=lhAJNwkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="http://www.linkedin.com/in/xuejianrong/"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <img src="avatar_circle.png">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                <!-- I'm interested in computer vision, machine learning, statistics, optimization, image processing, virtual reality, and computational photography. Much of my research is about inferring the physical world (shape, depth, motion, paint, light, colors, etc) from images. I have also worked in astronomy and biology. Representative papers are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">





          <tr>
            <td width="25%"><img src="images/cvpr2017.jpg" alt="cvpr2017" width="160" height="120" style="border-style: none">
            <td width="75%" valign="top">
              <p>
                <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Rong_Unambiguous_Text_Localization_CVPR_2017_paper.pdf" id="cvpr2017">
                  <papertitle>3D Self-Portraits</papertitle>
                </a>
                <br>
                <strong>Xuejian Rong</strong>, <a href="http://www.evouga.com/">Chucai Yi</a>, <a
                  href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018</em>, 2017 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
                <br>
                <a href="http://www.youtube.com/watch?v=DmUkbZ0QMCA">video</a> / <a href="http://shapify.me/">shapify.me</a> / <a
                  href="data/3DSP_siggraphAsia2013.bib">bibtex</a>
              </p>
              <p>
                Our system allows users to create textured 3D models of themselves in arbitrary poses using only a single 3D
                sensor.
                <br>
              </p>
            </td>
          </tr>











          <tr>
            <td width="25%"><img src="images/3DSP_160.jpg" alt="3DSP" width="160" height="120" style="border-style: none">
            <td width="75%" valign="top">
              <p>
                <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmbG1tOGIta3N1Wjg/view?usp=sharing" id="3DSP">
                  <papertitle>3D Self-Portraits</papertitle>
                </a>
                <br>
                <a href="http://www.hao-li.com/">Hao Li</a>, <a href="http://www.evouga.com/">Etienne Vouga</a>, Anton Gudym, <a
                  href="http://www.cs.princeton.edu/~linjiel/">Linjie Luo</a>, <strong>Jonathan T. Barron</strong>, Gleb Gusev
                <br>
                <em>SIGGRAPH Asia</em>, 2013 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
                <br>
                <a href="http://www.youtube.com/watch?v=DmUkbZ0QMCA">video</a> / <a href="http://shapify.me/">shapify.me</a> / <a
                  href="data/3DSP_siggraphAsia2013.bib">bibtex</a>
              </p>
              <p>
                Our system allows users to create textured 3D models of themselves in arbitrary poses using only a single 3D
                sensor.
                <br>
              </p>
            </td>
          </tr>


      <tr>
            <td width="25%"><img src="images/3DSP_160.jpg" alt="3DSP" width="160" height="120" style="border-style: none">
            <td width="75%" valign="top">
              <p>
                <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmbG1tOGIta3N1Wjg/view?usp=sharing" id="3DSP">
                  <papertitle>3D Self-Portraits</papertitle>
                </a>
                <br>
                <a href="http://www.hao-li.com/">Hao Li</a>, <a href="http://www.evouga.com/">Etienne Vouga</a>, Anton Gudym, <a
                  href="http://www.cs.princeton.edu/~linjiel/">Linjie Luo</a>, <strong>Jonathan T. Barron</strong>, Gleb Gusev
                <br>
                <em>SIGGRAPH Asia</em>, 2013 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
                <br>
                <a href="http://www.youtube.com/watch?v=DmUkbZ0QMCA">video</a> / <a href="http://shapify.me/">shapify.me</a> / <a
                  href="data/3DSP_siggraphAsia2013.bib">bibtex</a>
              </p>
              <p>
                Our system allows users to create textured 3D models of themselves in arbitrary poses using only a single 3D
                sensor.
                <br>
              </p>
            </td>
          </tr>
        
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Course Projects</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/prl.jpg" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing">
                  <papertitle>Parallelizing Reinforcement Learning</papertitle>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>, <a href="http://www.cs.berkeley.edu/~nickjhay/">Nicholas J. Hay</a>, 2009
                <p>
                  <br> Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.
                </p>
              </p>
            </td>
          </tr>
        </table>
        
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/pacman.jpg" alt="pacman" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">
                  <papertitle>CS188 - Fall 2010 (GSI)</papertitle>
                </a>
                <br>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">
                  <papertitle>CS188 - Spring 2011 (GSI)</papertitle>
                </a>
                <br>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
            </td>
          </tr>
        </table>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
