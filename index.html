<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      /* Sans: Lato, Fira Sans, Fira Sans Condensed;  Serif Sans: Merriweather, Lora, Galdeano*/
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 16px
    }
    
    strong {
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    heading {
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700
    }
    
    name {
      font-family: 'Galdeano', Verdana, Helvetica, sans-serif;
      font-size: 36px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/ico" href="favicon.ico">
  <title>Xuejian Rong</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href="https://fonts.googleapis.com/css?family=Galdeano" rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Xuejian Rong</name>
              </p>
              <p>I am a researcher at <a href="http://media-lab.ccny.cuny.edu">Media Lab</a> of <a href="http://www.ccny.cuny.edu">The City College</a>, <a href="http://cuny.edu">City University of New York</a>, where I work in the intersection of Deep Learning, Computer Vision, and Image Processing. I am finishing a Ph.D. advised by Prof. <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>. Currently my research interests mainly focus on inference and learning for scene text detection and recognition in the wild, and visual-linguistic understanding on scene text Images.
              </p>
              <p>
                My thesis proposal is entitled "<a href="https://www.dropbox.com/s/s4dlcuqwt3vml3x/2nd.Exam.Xuejian.Rong.4.28.17.slides.pdf?dl=0">Deep Features for Context-aware Text Extraction</a>" and the slides are available as linked. I interned at <a href="https://new.siemens.com/global/en/company/innovation/corporate-technology.html">Siemens Corporate Research</a> and worked on the visual representation learning for novel view synthesis. I obtained the B.E. degree from <a href="http://iao.nuaa.edu.cn">Nanjing University of Aeronautics and Astronautics</a> at 2013.
              </p>
              <p align=center>
                <a href="mailto:xrong@ccny.cuny.edu">Email</a> &nbsp/&nbsp
                <a href="resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=lhAJNwkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="http://www.linkedin.com/in/xuejianrong/"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <img src="avatar_circle.png">
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research (in maintenance)</heading>
              <p>
                <!-- I'm interested in computer vision, machine learning, statistics, optimization, image processing, virtual reality, and computational photography. Much of my research is about inferring the physical world (shape, depth, motion, paint, light, colors, etc) from images. I have also worked in astronomy and biology. Representative papers are <span class="highlight">highlighted</span>. -->
                <strong>News:</strong>
                <br>
                <ul>
                <li> Will work in Siemens Corporate Research as a research intern till December 2018. </li>
                <br>
                <li> Thanks to NVIDIA for supporting my research with the NVIDIA GPU Grant. </li>
                </ul>
              </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="25%"><img src="images/tpami_sub.png" alt="tpami_sub" width="160" height="120" style="border-style: none">
            <td width="75%" valign="top">
              <p>
                <a href="" id="tpami_sub">
                  <papertitle>Unambiguous Text Localization, Retrieval, and Recognition for Cluttered Scenes</papertitle>
                </a>
                <br>
                <strong>Xuejian Rong</strong>, <a href="http://yichucai.net">Chucai Yi</a>, <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli
                  Tian</a>
                <br>
                <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, Under Review
                <!-- <br>
                <a href="">code</a> / <a href="">data</a> -->
              </p>
              <p>
                Extended our previous CVPR paper as an End-to-End pipeline from scene text detection and retrieval to recognition.
                <br>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/arxiv18.png" alt="arxiv18" width="160" height="120" style="border-style: none">
            <td width="75%" valign="top">
              <p>
                <a href="https://arxiv.org/abs/1811.12297" id="arxiv18">
                  <papertitle>Incremental Scene Synthesis</papertitle>
                </a>
                <br>
                Benjamin Planche, <strong>Xuejian Rong</strong>, <a href="http://wuziyan.com/">Ziyan Wu</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, Harald Kosch, <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>, and Jan Ernst
                <br>
                <em>arXiv preprint arXiv:1811.12297</em>, Under Review
                <!-- <br>
                <a href="">code</a> / <a href="">data</a> -->
              </p>
              <p>
                To incrementally generates complete and consistent 2D or 3D scenes with learned scene priors, while real observations of an actual scene can be incorporated, and unobserved parts of the scene can be hallucinated. 
              </p>
              <p>
                Applications include autonomous agent exploration and few-shot learning.
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/tip_sub.png" alt="tip_sub" width="160" height="120" style="border-style: none">
            <td width="75%" valign="top">
              <p>
                <a href="" id="tip_sub">
                  <papertitle>Unambiguously Indicated Characterness for Referring Scene Text Segmentation</papertitle>
                </a>
                <br>
                <strong>Xuejian Rong</strong>, <a href="http://yichucai.net">Chucai Yi</a>, <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli
                  Tian</a>
                <br>
                <em>IEEE Transactions on Image Processing (TIP)</em>, Under Review
                <br>
                <a href="https://github.com/ionvision/TIP_CharRef">code</a> / <a href="https://github.com/ionvision/TIP_CharRef">dataset</a>
              </p>
              <p>
                Combining the power of both instance-level scene text segmentation and visual phrase grounding.
                <br>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/cvpr17.png" alt="cvpr17" width="160" height="120" style="border-style: none">
            <td width="75%" valign="top">
              <p>
                <a href="http://xrong.org/publications/cvpr17.pdf" id="cvpr17">
                  <papertitle>Unambiguous Text Localization and Retrieval for Cluttered Scenes</papertitle>
                </a>
                <br>
                <strong>Xuejian Rong</strong>, <a href="http://yichucai.net">Chucai Yi</a>, <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2017 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
                <br>
                <a href="https://github.com/ionvision/TextRef">code</a> / <a href="https://github.com/ionvision/TextRef">dataset</a>
              </p>
              <p>
              To utilize text instances for understanding natural scenes,
              we have proposed a framework that combines image-based
              text localization with language-based context description
              for text instances. 
              </p>
              <p>
              Specifically, we explore the task of unambiguous text localization and retrieval, to accurately localize a specific targeted text instance in a cluttered image given a natural language description that refers to it.
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/tcsvt17.png" alt="tcsvt17" width="160" height="120" style="border-style: none">
            <td width="75%" valign="top">
              <p>
                <a href="http://xrong.org/publications/tcsvt16.pdf" id="tcsvt17">
                  <papertitle>Evaluation of Low-Level Features for Real-World Surveillance Event Detection</papertitle>
                </a>
                <br>
                <a href="http://yangxian.info/">Yang Xian</a>, <strong>Xuejian Rong</strong>, <a href="http://xiaodongyang.org/">Xiaodong Yang</a>, <a
                  href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>
                <br>
                <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2017
                <br>
                <!-- <a href="">code</a> / <a href="">data</a> -->
              </p>
              <p>
                We evaluate several of the most commonly used low-level features for real-world surveillance event detection tasks.
                <br>
              </p>
            </td>
          </tr>

        <tr>
            <td width="25%"><img src="images/cyber17.png" alt="cyber17" width="160" height="120" style="border-style: none">
            <td width="75%" valign="top">
              <p>
                <a href="http://xrong.org/publications/cyber17.pdf" id="cyber17">
                  <papertitle>Assistive Indoor Navigation for the Visually Impaired in Multi-Floor Environments</papertitle>
                </a>
                <br>
                J. Pablo Munoz, <a href=https://robotlee2002.github.io/>Bing Li</a>, <strong>Xuejian Rong</strong>, <a href="https://ccny-ros-pkg.github.io/prof/jxiao.html">Jizhong Xiao</a>, <a
                  href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>, and Aris Arditi
                <br>
                <em>IEEE International Conference on Cyber Technology (CYBER)</em>, 2017 &nbsp <font color=#FF8080><strong>(Best Paper Award)</strong></font>
                <br>
                <!-- <a href="">code</a> / <a href="">data</a> -->
              </p>
              <p>
                Our system allows blind users to explore multi-floor environment with a wearable Tango device.  
                <br>
              </p>
            </td>
          </tr>


        <tr>
          <td width="25%"><img src="images/isvc16.png" alt="isvc2016" width="160" height="120" style="border-style: none">
          <td width="75%" valign="top">
            <p>
              <a href="http://xrong.org/publications/isvc16.pdf" id="isvc16">
                <papertitle>Guided Text Spotting for Assistive Blind Navigation in Unfamiliar Environments.</papertitle>
              </a>
              <br>
              <strong>Xuejian Rong</strong>, <a href=https://robotlee2002.github.io/>Bing Li</a>, <a href="https://ccny-ros-pkg.github.io/prof/jxiao.html">Jizhong
                Xiao</a>, Aris
              Arditi, and <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>
                <br>
                <em>The 12th International Symposium on Visual Computing (ISVC)</em>, 2017 &nbsp <font color=#FF8080><strong>(Oral)</strong></font>
                <br>
                <!-- <a href="">code</a> / <a href="">data</a> -->
            </p>
            <p>
              An assistive text spotting based navigation system is proposed, based on stroke-specific features and subsequent text tracking process.
              <br>
            </p>
          </td>
        </tr>


        <tr>
          <td width="25%"><img src="images/dsp16.png" alt="dsp16" width="160" height="120" style="border-style: none">
          <td width="75%" valign="top">
            <p>
              <a href="http://xrong.org/publications/dsp16.pdf" id="dsp16">
                <papertitle>Adaptive Shrinkage Cascades for Blind Image Deconvolution.</papertitle>
              </a>
              <br>
              <strong>Xuejian Rong</strong> and <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>
                <br>
                <em>IEEE International Conference on Digital Signal Processing (DSP)</em>, 2016 &nbsp <font color=#FF8080><strong>(Oral)</strong></font>
                <br>
                <!-- <a href="">code</a> / <a href="">data</a> -->
            </p>
            <p>
              A framework is proposed to deconvolve blind image with patch-wise prior and adaptive shrinkage cascades.
              <br>
            </p>
          </td>
        </tr>


        <tr>
          <td width="25%"><img src="images/rsuad16.png" alt="rsuad16" width="160" height="120" style="border-style: none">
          <td width="75%" valign="top">
            <p>
              <a href="http://xrong.org/publications/rsuad16.pdf" id="rsuad16">
                <papertitle>Recognizing Text-based Traffic Guide Panels with Cascaded Localization Network</papertitle>
              </a>
              <br>
              <strong>Xuejian Rong</strong>, and <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli
                Tian</a>
              <br>
              <em>ECCV Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving (CVRSUAD)</em>, 2016
              <br>
              <a href="http://xrong.org/publications/rsuad16_poster.pdf">poster</a> / <a href="http://media-lab.ccny.cuny.edu/wordpress/Code/TGPT.zip">dataset</a>
            </p>
            <p>
              A top-down framework is introduced for automatic localization and recognition of text-based traffic guide panels captured by car-mounted cameras from natural scene images.
              <br>
            </p>
          </td>
        </tr>


        <tr>
          <td width="25%"><img src="images/icmr16.png" alt="icmr16" width="160" height="120" style="border-style: none">
          <td width="75%" valign="top">
            <p>
              <a href="http://xrong.org/publications/icmr16.pdf" id="icmr16">
                <papertitle>Region Trajectories for Video Semantic Concept Detection</papertitle>
              </a>
              <br>
              Yuancheng Ye, <strong>Xuejian Rong</strong>, and <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>
              <br>
              <em>ACM International Conference on Multimedia Retrieval (ICMR)</em>, 2016
              <br>
              <!-- <a href="">code</a> / <a href="">data</a> -->
            </p>
            <p>
            We introduce an algorithm based on region trajectories to establish the connections between object localization in individual frames and video sequences.
            </p>
          </td>
        </tr>

        <tr>
          <td width="25%"><img src="images/acvr16.png" alt="acvr16" width="160" height="120" style="border-style: none">
          <td width="75%" valign="top">
            <p>
              <a href="http://xrong.org/publications/acvr16.pdf" id="acvr16">
                <papertitle>ISANA: Wearable Context-Aware Indoor Assistive Navigation with Obstacle Avoidance for the Blind</papertitle>
              </a>
              <br>
              <a href=https://robotlee2002.github.io/>Bing Li</a>, J. Pablo Munoz, <strong>Xuejian Rong</strong>, <a href="https://ccny-ros-pkg.github.io/prof/jxiao.html">Jizhong
                  Xiao</a>, <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>, and Aris
                Arditi
                <br>
                <em>ECCV Workshop on Assistive Computer Vision and Robotics (ACVR)</em> 2016
                <br>
                <!-- <a href="">code</a> / <a href="">data</a> -->
            </p>
            <p>
              We presented a novel mobile wearable context-aware indoor maps and navigation system with obstacle avoidance for the blind.
              <br>
            </p>
          </td>
        </tr>

        <tr>
          <td width="25%"><img src="images/robio15.png" alt="robio15" width="160" height="120" style="border-style: none">
          <td width="75%" valign="top">
            <p>
              <a href="http://xrong.org/publications/robio15.pdf" id="robio15">
                <papertitle>Assisting Blind People to Avoid Obstacles: An Wearable Obstacle Stereo Feedback System based on 3D Detection</papertitle>
              </a>
              <br>
              <a href=https://robotlee2002.github.io/>Bing Li</a>, Xiaochen Zhang, J. Pablo Munoz, <a href="https://ccny-ros-pkg.github.io/prof/jxiao.html">Jizhong
                  Xiao</a>, <strong>Xuejian Rong</strong>, and <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>
                <br>
                <em>IEEE International Conference on Robotics and Biomimetics (ROBIO)</em> 2015
                <br>
                <!-- <a href="">code</a> / <a href="">data</a> -->
            </p>
            <p>
              A wearable Obstacle Stereo Feedback (OSF) System for the Blind people based on 3D space obstacle detection is presented to assist the navigation.
              <br>
            </p>
          </td>
        </tr>


        <tr>
          <td width="25%"><img src="images/trecvid15.png" alt="trecvid15" width="160" height="120" style="border-style: none">
          <td width="75%" valign="top">
            <p>
              <a href="http://xrong.org/publications/trecvid15.pdf" id="trecvid15">
                <papertitle>CCNY at TRECVID 2015: Video Semantic Concept Localization</papertitle>
              </a>
              <br>
              Yuancheng Ye, <strong>Xuejian Rong</strong>, <a href="http://xiaodongyang.org/">Xiaodong Yang</a>, and <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli Tian</a>
                <br>
                <em>NIST TREC Video Retrieval Evaluation Workshop (TREVCID)</em> 2015
                <br>
                <!-- <a href="">code</a> / <a href="">data</a> -->
            </p>
            <p>
              We present a novel video-based object localization system,
              which is developed for the Semantic Localization task of TRECVID 2015.
            </p>
            <p>
              We won the <strong>1st</strong> place on this track. Extended to an ICMR 2016 paper.
            </p>
          </td>
        </tr>

        <tr>
          <td width="25%"><img src="images/trecvid14.png" alt="trecvid14" width="160" height="120" style="border-style: none">
          <td width="75%" valign="top">
            <p>
              <a href="http://xrong.org/publications/trecvid14.pdf" id="trecvid14">
                <papertitle>CCNY at TRECVID 2014: Surveillance Event Detection</papertitle>
              </a>
              <br>
              Yang Xian, <strong>Xuejian Rong</strong>, <a href="http://xiaodongyang.org/">Xiaodong Yang</a>, and <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli
                Tian</a>
              <br>
              <em>NIST TREC Video Retrieval Evaluation Workshop (TREVCID)</em> 2014
              <br>
              <a href="http://xrong.org/publications/trecvid14_poster.pdf">poster</a> / <a href="http://media-lab.ccny.cuny.edu/wordpress/Code/CCD-Dataset.zip">dataset</a>
            </p>
            <p>
              We present two video-based event detection systems for the Surveillance Event Detection (SED) task of TRECVID 2014.
            </p>
            <p>
              We won the <strong>3rd</strong> place on this track. Extended to a TCSVT paper.
            </p>
          </td>
        </tr>

        <tr>
          <td width="25%"><img src="images/icme14.png" alt="icme14" width="160" height="120" style="border-style: none">
          <td width="75%" valign="top">
            <p>
              <a href="http://xrong.org/publications/icme14.pdf" id="icme14">
                <papertitle>Scene Text Recognition in Multiple Frames based on Text Tracking</papertitle>
              </a>
              <br>
              <strong>Xuejian Rong</strong>, <a href="http://yichucai.net">Chucai Yi</a>, <a href="http://xiaodongyang.org/">Xiaodong Yang</a>, and <a href="https://scholar.google.com/citations?user=aAWeB4wAAAAJ&hl=en">Yingli
                Tian</a>
              <br>
              <em>IEEE International Conference on Multimedia and Expo (ICME)</em> 2014
              <br>
              <a href="http://xrong.org/publications/icme14_poster.pdf">poster</a>
            </p>
            <p>
              We proposed a multi-frame based scene text recognition method by tracking text regions in a video captured by a moving camera.
            </p>
          </td>
        </tr>

        <!-- </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Course Projects</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/prl.jpg" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing">
                  <papertitle>Title</papertitle>
                </a>
                <br>
                <strong>Xuejian Rong</strong>, <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>, <a href="http://www.cs.berkeley.edu/~nickjhay/">Nicholas J. Hay</a>, 2009
                <p>
                  <br> Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.
                </p>
              </p>
            </td>
          </tr>
        </table> -->
        
        
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/pacman.jpg" alt="pacman" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">
                  <papertitle>CS188 - Fall 2010 (GSI)</papertitle>
                </a>
                <br>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">
                  <papertitle>CS188 - Spring 2011 (GSI)</papertitle>
                </a>
                <br>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
            </td>
          </tr>
        </table>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table> -->
</body>

</html>
